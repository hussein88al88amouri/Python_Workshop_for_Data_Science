{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbf0dc22",
   "metadata": {},
   "source": [
    "# ML Workshop\n",
    "\n",
    "This notebook will provide a full workflow example in machine learning on the Iris dataset.\n",
    "- Outline:\n",
    "    - Utilities: dataset loader, plotting helpers, pipeline builder\n",
    "    - Model presets: several classifiers and a regression model\n",
    "    - Worked example: quick exploration, train/test split, scaling, train SVM & LogisticRegression, evaluate\n",
    "\n",
    "- How to use this cell:\n",
    "    - Read the inline comments and the Markdown prompts printed before each major step.\n",
    "    - Try swapping datasets (e.g., 'wine', 'digits') or models from MODEL_OPTIONS.\n",
    "    - Use build_classification_pipeline to create safe training pipelines and to add PCA.\n",
    "    - Exercises: hyperparameter tuning with GridSearchCV, cross-validation, plotting learning curves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd98e75",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc5c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell provides utilities, explanations, and a worked example (Iris).\n",
    "# Students: read the short Markdown prompts printed below to understand each step,\n",
    "# then modify the dataset, models, or parameters as exercises.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbbeca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment / display preferences\n",
    "# No need to modify this cell nor focus on it\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_columns', 50)\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd3d1d",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "These functions are ready implemented. \\\n",
    "You will have to use them later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a7a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: load popular small datasets by name\n",
    "def load_dataset(name=\"iris\"):\n",
    "    \"\"\"\n",
    "    Return (X, y, feature_names, target_names, dataset_object)\n",
    "    Supported names: 'iris' (default), 'wine', 'digits', 'breast_cancer', 'diabetes'\n",
    "    Note: 'diabetes' is a regression dataset.\n",
    "    Use this function when you want to quickly swap datasets for experiments.\n",
    "    \"\"\"\n",
    "    name = name.lower()\n",
    "    if name == \"iris\":\n",
    "        ds = datasets.load_iris()\n",
    "    elif name == \"wine\":\n",
    "        ds = datasets.load_wine()\n",
    "    elif name in (\"digits\", \"digit\"):\n",
    "        ds = datasets.load_digits()\n",
    "    elif name in (\"breast_cancer\", \"breastcancer\", \"cancer\"):\n",
    "        ds = datasets.load_breast_cancer()\n",
    "    elif name in (\"diabetes\",):\n",
    "        ds = datasets.load_diabetes()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {name}\")\n",
    "\n",
    "    X = ds.data\n",
    "    y = ds.target\n",
    "    # feature_names may not exist for all datasets (digits uses 'feature_names' sometimes absent)\n",
    "    feature_names = getattr(ds, \"feature_names\", [f\"f{i}\" for i in range(X.shape[1])])\n",
    "    target_names = getattr(ds, \"target_names\", np.unique(y).astype(str))\n",
    "    return X, y, feature_names, target_names, ds\n",
    "\n",
    "# Helper: quick pairplot (uses first 4 features by default to keep it readable)\n",
    "def quick_pairplot(X, y, feature_names, target_names=None, max_features=4):\n",
    "    \"\"\"\n",
    "    Quick visual diagnostic: pairwise scatter plots for up to `max_features`.\n",
    "    - X: feature array (n_samples, n_features)\n",
    "    - y: labels (n_samples,)\n",
    "    - feature_names: list of feature names (length n_features)\n",
    "    - target_names: optional list of class names for coloring\n",
    "    Use this when you want to inspect feature separation or spot outliers.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(X[:, :max_features], columns=feature_names[:max_features])\n",
    "    df['target'] = y\n",
    "    if target_names is not None:\n",
    "        # map numeric targets to their names for nicer legends\n",
    "        df['target_name'] = df['target'].map(lambda t: target_names[t] if t < len(target_names) else str(t))\n",
    "        sns.pairplot(df, hue='target_name', corner=True)\n",
    "    else:\n",
    "        sns.pairplot(df, hue='target', corner=True)\n",
    "    plt.suptitle(\"Pairplot (first {} features)\".format(min(max_features, X.shape[1])), y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "# Helper: plot confusion matrix nicely\n",
    "def plot_confusion(cm, labels, ax=None, cmap=\"Blues\", title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Nicely formatted confusion matrix using seaborn heatmap.\n",
    "    - cm: confusion matrix (2D array)\n",
    "    - labels: display labels for axes\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Helper: plot learning curve\n",
    "def plot_learning_curve(estimator, X, y, scoring=None, cv=5, train_sizes=np.linspace(0.1, 1.0, 5), title=\"Learning Curve\"):\n",
    "    \"\"\"\n",
    "    Plot a learning curve (training vs cross-validation score) as training data size increases.\n",
    "    Useful to diagnose high variance (overfitting) vs high bias (underfitting).\n",
    "    \"\"\"\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring=scoring, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Helper: generic pipeline builder for classification\n",
    "def build_classification_pipeline(model, use_pca=False):\n",
    "    \"\"\"\n",
    "    Build a standard pipeline:\n",
    "    - Impute missing values (mean)\n",
    "    - Standard Scaler\n",
    "    - Optional PCA (2 components for quick visualization)\n",
    "    - Classifier (model)\n",
    "    Use this function to ensure consistent preprocessing across experiments.\n",
    "    \"\"\"\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    "    if use_pca:\n",
    "        steps.append((\"pca\", PCA(n_components=2, random_state=RANDOM_STATE)))\n",
    "    steps.append((\"clf\", model))\n",
    "    return Pipeline(steps)\n",
    "\n",
    "# Quick list of model choices for students to try\n",
    "MODEL_OPTIONS = {\n",
    "    \"svm_linear\": SVC(kernel=\"linear\", probability=True, random_state=RANDOM_STATE),\n",
    "    \"svm_rbf\": SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE),\n",
    "    \"logistic\": LogisticRegression(max_iter=500, random_state=RANDOM_STATE),\n",
    "    \"decision_tree\": DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    \"knn\": KNeighborsClassifier(),\n",
    "    \"linear_regression\": LinearRegression(),  # for regression tasks\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning workshop helper loaded.**\n",
    "- Use `load_dataset(name)` to load data. Examples: `'iris'`, `'wine'`, `'digits'`, `'breast_cancer'`, `'diabetes'`.\n",
    "- Use `quick_pairplot(X, y, feature_names)` to visualize feature relationships (uses seaborn).\n",
    "- Use `build_classification_pipeline(model, use_pca=False)` to get a safe training pipeline.\n",
    "- Use `plot_learning_curve(estimator, X, y, cv=5)` to inspect learning behavior.\n",
    "- `MODEL_OPTIONS` contains several model presets you can try.\n",
    "Exercises / TODO:\n",
    "- Try different datasets and compare model performance.\n",
    "- Tune hyperparameters using `GridSearchCV`.\n",
    "- Try regression on `'diabetes'` with `linear_regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises â€” Start Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84936f0d",
   "metadata": {},
   "source": [
    "## Worked Example: Iris Dataset\n",
    "\n",
    "We load Iris, inspect basic statistics, visualize, preprocess, train two classifiers, and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b325c42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a Dataset\n",
    "# We'll start with the classic Iris dataset.\n",
    "# Note: replace 'iris' below with 'wine' or 'digits' to try other datasets.\n",
    "\n",
    "dataset = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.data\n",
    "y = dataset.target\n",
    "feature_names = dataset.feature_names\n",
    "target_names = dataset.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6af508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Print dataset description. Number of features, samples, classes, feature names, target names.\n",
    "# Write you code Here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f64eb",
   "metadata": {},
   "source": [
    "### Data Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c18d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Explore the Data\n",
    "# TODO: Create a DataFrame from X and y, print first 5 rows and class distribution.\n",
    "# TODO: continue the scatter plot code below to use variables instead of hardcoded values.\n",
    "df = pd.DataFrame(___, ___)\n",
    "df['target'] = ___\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.___())\n",
    "print('Class distribution')\n",
    "print(df['___'].___())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2585ea",
   "metadata": {},
   "source": [
    "### Scatter plot of first two features\n",
    "This plot shows how the first two features separate the classes. Try other pairs or `quick_pairplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a4d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Visualize the Data\n",
    "# TODO: continue the scatter plot code below to use variables instead of hardcoded values.\n",
    "# You can also try other pairs of features by changing the indices [, 0] and [, 1]\n",
    "plt.figure(figsize=(8,6))\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(X[y == ___, 0], X[y == ___, 1], label=___)\n",
    "plt.xlabel(feature_names[0])\n",
    "plt.ylabel(feature_names[1])\n",
    "plt.legend()\n",
    "plt.title(\"Iris Dataset - Feature Scatter Plot\")\n",
    "plt.show()\n",
    "\n",
    "# TODO: use quick pairplot by uncommenting:\n",
    "# quick_pairplot(X, y, feature_names, target_names, max_features=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c981ca",
   "metadata": {},
   "source": [
    "### Preprocessing: train/test split and scaling\n",
    "\n",
    "We will stratify the split to preserve class proportions. Scaling is important for SVM and many other algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Prepare the Data\n",
    "# Split into train and test sets\n",
    "# TODO: continue the scatter plot code below to use variables instead of hardcoded values.\n",
    "# Make sure to specify test_size (e.g., 0.2 for 20% test).\n",
    "X_train, X_test, y_train, y_test = train_test_split(__, __, test_size=__, random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a11a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Standardize features\n",
    "# TODO: continue the scatter plot code below to use variables instead of hardcoded values.\n",
    "# Use StandardScaler to fit on training data and transform both train and test.\n",
    "# Call the methods fit_transform and transform appropriately.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.__(__) # call fit_transform on training data\n",
    "X_test_scaled = scaler.__(__) # call transform on test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fab959",
   "metadata": {},
   "source": [
    "### Train classifiers: SVM (linear) and Logistic Regression\n",
    "\n",
    "Change hyperparameters or swap models to see how performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76d9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train a Model - Support Vector Machine (SVM)\n",
    "svm_clf = SVC(kernel='linear', random_state=RANDOM_STATE)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1920e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Train a Model - Logistic Regression\n",
    "logreg_clf = LogisticRegression(max_iter=200, random_state=RANDOM_STATE)\n",
    "logreg_clf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e74c0",
   "metadata": {},
   "source": [
    "### Evaluation: accuracy and classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde155bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluate the Models\n",
    "svm_pred = svm_clf.predict(X_test_scaled)\n",
    "logreg_pred = logreg_clf.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nSVM Accuracy:\", accuracy_score(y_test, svm_pred))\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_pred))\n",
    "\n",
    "print(\"\\nSVM Classification Report:\")\n",
    "print(classification_report(y_test, svm_pred, target_names=target_names))\n",
    "\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, logreg_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab069a6",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "\n",
    "Inspect which classes are confused. Use `plot_confusion()` for a nicer heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eef456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Confusion Matrix Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "cm_svm = confusion_matrix(y_test, svm_pred)\n",
    "cm_logreg = confusion_matrix(y_test, logreg_pred)\n",
    "\n",
    "# Using imshow is fine but plot_confusion gives a nicer output; showing both approaches is educational.\n",
    "axes[0].imshow(cm_svm, cmap='Blues')\n",
    "axes[0].set_title('SVM Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('True')\n",
    "\n",
    "axes[1].imshow(cm_logreg, cmap='Greens')\n",
    "axes[1].set_title('Logistic Regression Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('True')\n",
    "\n",
    "for ax, cm in zip(axes, [cm_svm, cm_logreg]):\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Also demonstrate the helper heatmap for one of them:\n",
    "display(Markdown(\"Helper heatmap (SVM):\"))\n",
    "plot_confusion(cm_svm, target_names, title=\"SVM Confusion Matrix (heatmap)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde2eb5",
   "metadata": {},
   "source": [
    "### Summary comparison\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de03b318",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# 10. Compare Models\n",
    "print(\"SVM vs Logistic Regression:\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, svm_pred):.2f}\")\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_score(y_test, logreg_pred):.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcc7bc",
   "metadata": {},
   "source": [
    "## Next Steps / Exercises - On you Own\n",
    "\n",
    "- Try `load_dataset('wine')` or `load_dataset('digits')` and repeat the workflow.\n",
    "- Use `build_classification_pipeline(MODEL_OPTIONS['svm_rbf'], use_pca=True)` and train with cross-validation.\n",
    "- Perform grid search over hyperparameters (e.g. `C` and `kernel` for SVC) using `GridSearchCV`.\n",
    "- Plot learning curves with `plot_learning_curve()` to diagnose bias/variance.;\n",
    "- For regression: load `'diabetes'` and try `linear_regression` with mean squared error and R^2 metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
